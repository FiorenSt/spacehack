{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Classification Transients\n",
    "==============\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/turanbulmus/spacehack/blob/main/01 - LLM Classification Transients.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fturanbulmus%2Fspacehack%2Fmain%2F01%20-%20LLM%20Classification%20Transients.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/turanbulmus/spacekhack/main/01%20-%20LLM%20Classification%20Transients.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/turanbulmus/spacehack/blob/main/01%20-%20LLM%20Classification%20Transients.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Authors   | Turan Bulmus,  Fiorenzo Stoppa|\n",
    "| Last updated | 2025 03 20: Final Publication |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook showcases the potential of multimodal models like Gemini 1.5 Pro for tackling few-shot object classification problems. We'll explore how this advanced model can analyze telescope images, compare them against reference and difference images, and classify them as \"real\" or \"bogus\" solely based on visual information and human-provided instructions. By the end of this notebook, you'll gain insights into:\n",
    "\n",
    "* How Gemini 1.5 Pro can perform few-shot classification tasks without explicit training.\n",
    "* How Gemini 1.5 Pro not only classifies images but also provides human readable explanations for the classifications.\n",
    "* The power of multimodal models to understand and reason about visual content.\n",
    "* Building effective prompts for complex image analysis tasks.\n",
    "\n",
    "TODO: Add link to the paper\n",
    "\n",
    "This notebook demonstrates these with the following steps:\n",
    "\n",
    "1. **Import Libraries and Build Functions:** We start by importing the necessary libraries for image loading, processing, and evaluation. We also define functions to facilitate the analysis and visualization of results.\n",
    "2. **Build System Instructions for the Prompt:** A well-crafted prompt is crucial for guiding Gemini 1.5 Pro towards the desired output. We carefully define instructions outlining the task, the type of images, and the desired classification outcome.\n",
    "3. **Load the Dataset:** We load a collection of telescope images, including reference, difference, and new images, to be used for the classification task.\n",
    "4. **Run Gemini 1.5 Pro with the Prompt:** We execute the prompt with Gemini 1.5 Pro, iterating over 3000+ samples from the dataset to get classifications for each image.\n",
    "5. **Evaluate Model Performance:** We evaluate the model's performance using metrics like a confusion matrix, precision, recall, and accuracy. These metrics provide insights into the model's ability to correctly classify \"real\" and \"bogus\" images. Finally, we visualize the results for better understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using This Notebook\n",
    "\n",
    "Colab is recommended for running this notebook, but it can run in any iPython environment where you can connect to Google Cloud, install pip packages, etc.\n",
    "\n",
    "If you're running outside of Colab, depending on your environment you may need to install pip packages (at the very least `pandas` and `tabulate`) that are included in the Colab environment by default but are not part of the Python Standard Library--try pipping the library name of any imports that fail. You'll also notice some comments in code cells that look like \"@something\"; these have special rendering in colab, but you aren't missing out on any content or important functionality.\n",
    "\n",
    "This notebook uses the following Google Cloud services and resources:\n",
    "\n",
    "* [Vertex AI Gemini Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models)\n",
    "* [Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments)\n",
    "* [Google Cloud BigQuery](https://cloud.google.com/bigquery/docs/introduction)\n",
    "\n",
    "This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.12.3\n",
    "* [google-cloud-aiplatform](https://pypi.org/project/google-cloud-aiplatform/) version = 1.70.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Tips\n",
    "\n",
    "1. This notebook uses Generative AI cababilities. Re-running a cell that uses Generative AI capabilities may produce similar but not identical results.\n",
    "\n",
    "TODO: Add tips regarding: Quotas, possible errors (429 etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages and restart runtime \n",
    "Install the required packages for code to run. These packages are included in the requirements.txt file.\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step.\n",
    "\n",
    "The code assumes that the packages are already installed hence the following code is commented out. If you are running this notebook for the first time, please uncomment the next block and run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt --quiet\n",
    "\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate your notebook environment\n",
    "\n",
    "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
    "\n",
    "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
    "\n",
    "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab authentication\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    print('Authenticated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud environment variables information and initialize Clients\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and the following:\n",
    "* [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
    "* [Enable the Big Query API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables will be used throughout the code below. Please find the meaning of each variable in the documentation below:\n",
    "\n",
    "* [PROJECT_ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects): The ID of the GCP project which will host the \n",
    "* [LOCATION](https://cloud.google.com/about/locations): The location where the computational resources will be run. Please also check the availability of Gemini models in the associated regions from [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations).\n",
    "* [EXPERIMENT_NAME](https://cloud.google.com/vertex-ai/docs/experiments/create-manage-exp-run): The name of the Vertex AI experiment. This will ensure tracebility of the experiement in the Vertex AI Environment\n",
    "* [DATASET_ID](https://cloud.google.com/bigquery/docs/datasets-intro): The code will run a batch process for various LLM calls. The results and the input of these runs will be stored at a BigQuery table. The instructions to create a dataset can be found [here](https://cloud.google.com/bigquery/docs/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-67e6515e-b143-4f35-a7db-0d230d781986\" href=\"#view-view-vertex-resource-67e6515e-b143-4f35-a7db-0d230d781986\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-67e6515e-b143-4f35-a7db-0d230d781986');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROJECT_ID = \"<YOUR_PROJECT_ID>\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"supernovadetection\" # @param {type:\"string\"}\n",
    "# Make sure that dataset is created in Big Query\n",
    "DATASET_ID = \"spacehack\" # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initiate Vertex AI and BigQuery clients\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries that will be used throughout the code. Please note that the last import (i.e. helper_functions) is coming originated from the source code in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "import os, datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from helper_functions import build_run_batch, if_tbl_exists, create_ex, save_picture, save_prompt, build_experiment_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build System Instructions and The Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the System Instructions\n",
    "System instructions for an LLM (Large Language Model) are a set of instructions given at the very beginning of a conversation (or as a persistent setting) that tell the model how to behave, what role to assume, and what constraints to operate under.\n",
    "\n",
    "Think of it as the LLM's \"mission statement\" for the entire interaction. They can:\n",
    "\n",
    "* Define the LLM's persona: \"You are a helpful and friendly chatbot.\"\n",
    "* Set response style: \"Answer concisely and use bullet points.\"\n",
    "* Specify the task: \"You are an expert in data science and will help users with their coding problems.\"\n",
    "* Impose limitations: \"Do not provide medical advice.\"\n",
    "* Provide context: \"The user is a beginner in Python programming.\"\n",
    "\n",
    "Since one can not include images to system instructions (yet) below will only consist of text portion of the system instructions. \n",
    "For this project we are introducing four main parts to the System instructions:\n",
    "\n",
    "1) Persona: Highlighting the persona of the model. Eg. \"You are an experienced astrophysicist...\"\n",
    "2) Instructions: This is set of instructions that are also used as guidelines for human classifiers.\n",
    "3) Task: Explaining the task that the LLM model needs to conduct.\n",
    "4) Method: Fine grained instructions on how to conduct the task outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = \"\"\"<PERSONA>\n",
    "You are an experienced astrophysicist, and your task is to classify astronomical transients into Real or Bogus based on a given set of 3 images. You have seen thousands of astronomical images during your lifetime and you are very good at making this classification by looking at the images and following the instructions.\n",
    "</PERSONA>\"\"\"\n",
    "\n",
    "TASK = \"\"\"<TASK>\n",
    "Your task is to read the INSTRUCTIONS, look at the 3 images (New, Reference and Difference images) and classify if the source at the centre of the cutout and inside the red circle is a Real or Bogus astronomical transient. Provide your thought process to explain how you reasoned to provide the response. Respond in json format\n",
    "</TASK>\\n\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTIONS = \"\"\"\\n<INSTRUCTIONS>\n",
    "**1. Purpose**\n",
    "Help vet astronomical data for the Real/Bogus classification. The goal is for you to use your expertise to distinguish between real and bogus sources. \n",
    "\n",
    "**2. Information Provided**\n",
    "You will be shown three astronomical image cutouts:\n",
    "a) **New Image:** The newest image centered at the location of the suspected transient source. \n",
    "b) **Reference Image:** A reference image from the same telescope of the same part of the sky to be used for comparison. It shows if the source was already there in the past or not.\n",
    "c) **Difference Image:** The residual image after the new and reference images are subtracted. Real sources should appear in this cutout as circular objects with only positive (white pixels) or only negative (black pixels) flux. \n",
    "\n",
    "**3. Criteria for Classification**\n",
    "- **Real Source:** \n",
    "  - **Shape:** Circular shape at the center of the cutout with a visual extent of ~5-10 pixels, varying with focus conditions.\n",
    "  - **Brightness:** Positive flux (white pixels) in either the new or reference image. Positive or negative flux in the Difference image. \n",
    "  - **Variability:** The source at the center can fade or brighten between the new and reference images, appearing as positive or negative in the Difference image.\n",
    "  - **Presence:** The source may (dis)appear between the new and reference images. A source may also appear on top of an underlying source (e.g., supernova on a galaxy).\n",
    "\n",
    "- **Bogus Source:** \n",
    "  - **Shape:** Non-circular shape (e.g., elongated). This includes irregular shapes, positive or negative, like streaks or lines caused by cosmic-rays, diffraction spikes and cross-talk.\n",
    "  - **Brightness:** Negative flux (black pixels) at the center of the cutout in either the new or reference image. The source at the center can never be negative in the New or Reference image, only in the Differnece.\n",
    "  - **Misalignment:** If the source in the New and Reference images is misaligned, it will show a Yin-Yang pattern (both white and black) in the Difference image.\n",
    "\n",
    "**4. Additional Guidance** \n",
    "- **Contextual Information:** Focus on the source at the center of the cutouts inside the red circle, but consider nearby sources to diagnose potential problems.  \n",
    "- **Examples:** Refer to provided visual examples of real and bogus sources to aid in identification. \n",
    "- **Judgment Criteria:** For ambiguous cases or borderline scenarios, consider the overall context and consistency with known characteristics of real and bogus sources.\n",
    "</INSTRUCTIONS>\"\"\"\n",
    "\n",
    "\n",
    "METHOD = \"\"\"<METHOD>\n",
    "1. **Focus on the Red Circle**: Start by examining the source located at the center of the cutout and inside the red circle. The images are prepared so that the source of interest is clearly marked for you to analyze.\n",
    "\n",
    "2. **Analyze Each Image Individually**:\n",
    "   - **New Image**: Check for the presence, shape, and brightness of the source in the new image.\n",
    "   - **Reference Image**: Compare the source's properties in the reference image to those in the new image.\n",
    "   - **Difference Image**: Observe the residuals that result from subtracting the reference image from the new image. Look for patterns (circular, positive/negative flux) that match characteristics of Real or Bogus sources.\n",
    "\n",
    "3. **Evaluate Features**:\n",
    "   - Examine the shape, brightness, and other relevant features (e.g., artifacts, misalignments) of the source in each image.\n",
    "   - Determine if these features are consistent with a Real or Bogus classification based on the criteria provided in the instructions.\n",
    "\n",
    "4. **Consider Relationships Between Images**:\n",
    "   - Compare the new, reference, and difference images to understand any changes in the source over time.\n",
    "   - Look for discrepancies or confirmations that might support or contradict a particular classification.\n",
    "\n",
    "5. **Employ a Chain-of-Thought Reasoning**:\n",
    "   - Clearly outline each observation you make and explain how it contributes to your decision-making process.\n",
    "   - If you find any contradictions or ambiguous features, acknowledge them and provide reasoning for your final decision.\n",
    "\n",
    "6. **Assign an Interest Score**:\n",
    "   - After determining if the source is Real or Bogus, assign an appropriate interest score:\n",
    "     - 'No interest' for Bogus sources.\n",
    "     - 'Low interest' for variable transients.\n",
    "     - 'High interest' for explosive transients.\n",
    "\n",
    "7. **Prepare the Final Output in JSON Format**:\n",
    "   - Format your response as a JSON object containing:\n",
    "     - The classification ('Real' or 'Bogus').\n",
    "     - An explanation detailing your thought process and observations.\n",
    "     - The assigned interest score.\n",
    "\n",
    "8. **Example Output**:\n",
    "   - Refer to the provided examples to see the expected format and detail level of your response.\n",
    "</METHOD>\n",
    "\"\"\"\n",
    "\n",
    "# Collapse the System Instructions into a single variable\n",
    "stat_prompt = PERSONA + TASK + INSTRUCTIONS + METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Prompt\n",
    "\n",
    "In this part, we will build the prompt (the dynamic) part of each LLM call for each of the sample in the dataset. The process for building these examples would be as follows:\n",
    "\n",
    "1) Download the raw image data (.npy files) and the labels (.csv) file associated with the images\n",
    "2) Upload the labels to BigQuery\n",
    "3) Seperate example set images (i.e. the few shot examples - 7 Real and 7 Bogus examples) that will be sent as a part of each LLM call. They will be stored under sample_indexes.\n",
    "4) Convert the raw .npy files to pictures so that they can be sent to Gemini and store them under the data folder.\n",
    "5) For each example in sample_indexes, define the expected output for class, explanation and interest_score.\n",
    "6) Store the dynamic part of the prompt under the variable EXAMPLES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "file_path_data = 'data/new_data.npy'\n",
    "file_path_labels_csv = 'data/new_labels.csv'\n",
    "\n",
    "triplets = np.load(file_path_data)\n",
    "\n",
    "# Load labels from CSV file\n",
    "labels_df = pd.read_csv(file_path_labels_csv)\n",
    "\n",
    "# Upload Labels to BigQuery\n",
    "labels_id = \"MeerLICHT_labels_df\"\n",
    "labels_ref = bq_client.dataset(DATASET_ID).table(labels_id)\n",
    "create_table_flag = if_tbl_exists(bq_client, labels_ref)\n",
    "\n",
    "if create_table_flag != True:\n",
    "    bq_client.load_table_from_dataframe(labels_df, labels_ref)\n",
    "\n",
    "# Define sample indexes for example images\n",
    "sample_indexes = [0, 1, 3, 4, 8, 48, 77, 592, 685, 1179, 1180, 1181, 1191, 1193, 3216]\n",
    "\n",
    "# Save example pictures\n",
    "for i in sample_indexes:\n",
    "    save_picture(triplets, i, True)\n",
    "\n",
    "# Identify valid indices by removing any with corrupt data\n",
    "valid_indices = np.where(~np.isnan(triplets).any(axis=(1, 2, 3)))[0]\n",
    "\n",
    "# Exclude sample_indexes from the prediction set\n",
    "batch_index = np.setdiff1d(valid_indices, sample_indexes)\n",
    "\n",
    "# Save prediction pictures\n",
    "for t in batch_index:\n",
    "    save_picture(triplets, t, False)\n",
    "\n",
    "print(f\"Saved {len(batch_index)} prediction pictures excluding examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRIPTION INDEX 0:\n",
    "desc1 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"In the New image, a diffraction spike is observed near the center. The Reference image also shows a diffraction spike at the same location. In the Difference image, a negative residual of the bright diffraction spike from the Reference image is clearly visible. The consistent presence of diffraction spikes in all three images, without a clear circular source, confirms that this is a bogus source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1:\n",
    "desc2 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"In the New image, a negative elongated artifact is present at the center. The Reference image does not show any source at the same location. In the Difference image, the same negative artifact appears, which results from the negative clump of pixels in the New image. Since a real source cannot be negative in the New image, this is classified as a bogus source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 3:\n",
    "desc3 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"In the New image, the source appears as a streak of several bright pixels and is not circular. The Reference image shows no source at the same location. The Difference image shows the same streak of pixels as in the New image. The sharp, streak-like appearance in the New image suggests that this is most likely a cosmic ray rather than a real source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 4:\n",
    "desc4 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"The New image does not have any source at the centre of the cutout. The Reference image shows a source appearing as a streak of a few bright pixels, which is not circular. The difference image shows the negative residual of the same streak present in the Reference image. This is too sharp to be a real source and is likely a cosmic ray that was not flagged during the creation of the Reference image.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 8:\n",
    "desc5= {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"No source is present in the New image. In the Reference image, a source appears as a negative circular object. The Difference image presents a faint positive residual of the source in the Reference image. A source cannot be negative in the Reference image, this is not a real source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 48:\n",
    "desc6 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"The New image does not have any source at the centre of the cutout. In the Reference image, the source appears very elongated. The Difference shows the same negative elongated source, supporting the conclusion that it a bogus source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 77:\n",
    "desc7 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"In the New image, a small elongated source is visible, surrounded by several other sources. The Reference image shows no source at the same location, but it does show all the other sources. In the Difference image, the residual is positive but its elongation confirms this is a bogus source.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 592:\n",
    "desc8 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"The New image shows a diffuse source at the center, aligned with a 45-degree diffraction spike from a bright source at the corner of the cutout. The Reference image also shows a diffraction spike and a similar blob. The Difference image displays a positive blob, indicating it is an artifact caused by the diffraction spike, which can produce blobs or irregular shapes.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 685:\n",
    "desc9 = {\n",
    "  \"class\": \"Bogus\",\n",
    "  \"explanation\": \"The New image shows no source at the center. The Reference image shows a faint positive trail cutting diagonally across the image with a circular source at the center, likely caused by a blinking object like an airplane or satellite. The Difference image displays both the trail and a negative blob, confirming the source is a non-astronomical artifact.\",\n",
    "  \"interest_score\": \"No interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 1179:\n",
    "desc10 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows a source at the center. The Reference image also shows the same source in the same location. The Difference image has a positive residual, indicating the source has brightened. This pattern suggests the source is a real variable star.\",\n",
    "  \"interest_score\": \"Low interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 1180:\n",
    "desc11 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows a source at the center. The Reference image also shows the same source in the same location. The Difference image has a negative residual, indicating the source has dimmed. This pattern suggests the source is a real variable star.\",\n",
    "  \"interest_score\": \"Low interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 1181:\n",
    "desc12 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows no source at the center. The Reference image shows a circular source in the same location. The Difference image displays a negative circular residual, consistent with a transient that has disappeared.\",\n",
    "  \"interest_score\": \"High interest\"\n",
    "}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 1191:\n",
    "desc13 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows a bright circular source at the center. The Reference image shows no source in the same location. The Difference image displays a positive circular residual, indicating a real explosive transient.\",\n",
    "  \"interest_score\": \"High interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1193:\n",
    "desc14 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows a source at the center. The Reference image also shows the same source in the same location. The Difference image displays a positive residual, indicating the source has brightened. A cosmic ray artifact is visible to the left, but the central source is unaffected and remains a valid transient.\",\n",
    "  \"interest_score\": \"Low interest\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 3216:\n",
    "desc15 = {\n",
    "  \"class\": \"Real\",\n",
    "  \"explanation\": \"The New image shows a source at the center, superimposed on a diffuse galaxy. The Reference image displays the galaxy but no source at the same location. The Difference image reveals a faint, positive circular feature, consistent with a supernova emerging within the galaxy.\",\n",
    "  \"interest_score\": \"High interest\"\n",
    "}\n",
    "\n",
    "descriptions = [str(desc1), str(desc2), str(desc3), str(desc4), str(desc5), str(desc6),str(desc7), str(desc8), str(desc9), str(desc10), str(desc11), str(desc12), str(desc13), str(desc14), str(desc15)]\n",
    "\n",
    "### Write the examples used in a readable format to be saved as a txt file for tracebility\n",
    "example_description = list(zip([\"DESCRIPTION INDEX: \" + str(x) for x in sample_indexes], descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 15 examples for the dynamic prompt\n",
    "EXAMPLES = [\"<EXAMPLES>\\n\"]\n",
    "for i in range(len(sample_indexes)):\n",
    "    \n",
    "    str_EX = f\"\"\"Example {i+1}:\n",
    "    \"\"\"\n",
    "    all_list = create_ex(sample_indexes[i], True)\n",
    "    all_list.insert(0, str_EX)\n",
    "    all_list.append(descriptions[i])\n",
    "    all\n",
    "    for k in all_list:\n",
    "        EXAMPLES.append(k)\n",
    "EXAMPLES.append(\"\\n</EXAMPLES>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the batch run\n",
    "\n",
    "After creating the system instructions and the prompt for each example, we will first create the experiment variables for tracebility and then initiate the batch process with the function ```build_run_batch``` function from ```helper_functions.py```\n",
    "\n",
    "```build_run_batch``` does the following:\n",
    "1. Constructs input and output table names based on the project ID and formatted datetime.\n",
    "2. Defines the table schema with 'request' (JSON) and 'index_no' (INTEGER) fields.\n",
    "3. Creates the input table in BigQuery if it doesn't exist.\n",
    "4. For index item in the batch_index:\n",
    "    - Constructs a dynamic prompt using the provided examples and the current index.\n",
    "    - Creates a batch data dataframe using the static prompt, dynamic prompt, and specified parameters.\n",
    "    - Uploads the dataframe to a GCS bucket\n",
    "    - Creates a Big query table using the data stored in GCS bucket. \n",
    "5. Generates a request.json file for batch processing.\n",
    "6. Sends the batch prediction job to the specified project.\n",
    "7. Waits until the batch prediction job concludes.\n",
    "8. Generate a Big Query table that processes the ```BatchPredictionJob```\n",
    "9. Download the table and exports as a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging the experiment\n",
    "\n",
    "## Prepare the variables\n",
    "timestamp = datetime.datetime.now()\n",
    "formatted_datetime = timestamp.strftime('%Y%m%d%H%M')\n",
    "\n",
    "## Log the experiments variables\n",
    "### Create the run name with timestamp\n",
    "run_name = \"run\" + formatted_datetime\n",
    "DESCRIPTION = \"\"\"This is the final run to be pushed to the repo highlighting reproducibility\"\"\" # @param {type:\"string\"}\n",
    "MODEL = \"gemini-1.5-pro-002\" # @param [gemini-1.5-pro-001\", \"gemini-1.5-flash-001\", \"gemini-1.0-pro-002\"]\n",
    "TEMPERATURE = .8 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "TOP_P = 1 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "PROMPT_FILE = save_prompt(stat_prompt + '\\n'.join([a + \"\\n\" + b + \"\\n\" for (a,b) in example_description]), run_name)\n",
    "\n",
    "# Build the experimentation variables\n",
    "exp_vars = build_experiment_vars(description=DESCRIPTION,prompt=PROMPT_FILE, model=MODEL, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "# # Start the run\n",
    "aiplatform.start_run(run_name)\n",
    "# # Log the experiment variables\n",
    "aiplatform.log_params(exp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and then the batch run requests\n",
    "pred_df = build_run_batch(bq_client, batch_index, labels_ref, PROJECT_ID, DATASET_ID, run_name, MODEL, stat_prompt, EXAMPLES, TEMPERATURE, TOP_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyze the results and conclude the experiment\n",
    "\n",
    "Once the batch run is completed the results are stored in pred_df dataframe. We will next evaluate the performance of the binary classification model by generating a confusion matrix and calculating key performance metrics.\n",
    "\n",
    "The evaluation will be conducted via:\n",
    "\n",
    "1.  **Visualize Model Performance:** Create and display a confusion matrix to visually represent the model's performance in classifying instances into two categories (likely \"Real\" and \"Bogus\").\n",
    "2.  **Calculate Performance Metrics:** Compute essential performance metrics (Accuracy, Precision, and Recall) based on the confusion matrix.\n",
    "\n",
    "Finally we will conclude the code by logging the metrics and ending the experiment run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix for the results\n",
    "plt.clf()\n",
    "# Adding this line to make sure that the ground truth data is reflected correctly in the pred_df dataframe\n",
    "pred_df = pd.merge(pred_df, labels_df[['index_no', 'label']], on='index_no', how='left')\n",
    "pred_cleaned = pred_df[(pred_df.predicted == \"Real\") | (pred_df.predicted == \"Bogus\")]\n",
    "cm = confusion_matrix(pred_cleaned.label, pred_cleaned.predicted)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "# Calculate TP, TN, FP, FN\n",
    "TP = cm[0][0]\n",
    "TN = cm[1][1]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "# Print the metrics\n",
    "print(f\"Accuracy is {(TN+TP)/(TN+TP+FP+FN)}\")\n",
    "print(f\"Precision is {TP/(TP+FP)}\")\n",
    "print(f\"Recall is {TP/(TP+FN)}\")\n",
    "\n",
    "#Save the data frame for reproducibility \n",
    "pred_df.to_csv(\"data/predictions_results.csv\", mode=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the KPI and conclude the experiment\n",
    "aiplatform.log_metrics(build_experiment_vars(accuracy=(TN+TP)/(TN+TP+FP+FN), precision=TP/(TP+FP), recall=TP/(TP+FN)))\n",
    "aiplatform.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "In this exercise we showed how to use Gemini models to distinguish between real astrophysical signals (e.g., explosive events, variable stars) and bogus imaging artifacts. This is the first demonstration of a successful application of an LLM to imaging data from optical transient surveys. \n",
    "\n",
    "In this part we will be cleaning after resources used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the resources. \n",
    "\n",
    "# Delete the BigQeury Tables\n",
    "bq_client.delete_table(labels_ref)\n",
    "bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.{run_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
