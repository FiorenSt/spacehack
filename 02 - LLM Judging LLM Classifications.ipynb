{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Judging LLM Classifications\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/turanbulmus/spacehack/blob/main/02 - LLM Judging LLM Classifications.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fturanbulmus%2Fspacehack%2Fmain%2F02%20-%20LLM%20LLM%20Judging%20LLM%20Classifications.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/turanbulmus/spacekhack/main/02%20-%20LLM%20LLM%20Judging%20LLM%20Classifications.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/turanbulmus/spacehack/blob/main/02%20-%20LLM%20LLM%20Judging%20LLM%20Classifications.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Authors   | Turan Bulmus,  Fiorenzo Stoppa|\n",
    "| Last updated | 2025 03 20: Final Publication |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook builds on top of the previous notebook and judges the results of the run obtained in the previous notebook. For each observeration, an LLM \"referee\" looks at the generated results and \"judges\" if the generated content is correct or not. By the end of this notebook, you will gain insights into:\n",
    "* How one can uses LLM to evaluate the generated results by an another LLM Model\n",
    "* Generate human readable evaluation metrics to be used by LLM to judge the results of an another LLM\n",
    "\n",
    "This notebook demonstrates these with the following steps:\n",
    "\n",
    "1. **Import Libraries and Build Variables:** Similar to the previous notebook, we start by importing the necessary libraries for image loading, processing and conduct the LLM calls. \n",
    "2. **Build System Instructions for the Prompt:** A well-crafted prompt is crucial for guiding Gemini 1.5 Pro towards the desired output. We carefully define instructions outlining the task, evaluation criteria and the method to do so.\n",
    "3. **Load the Dataset:** We load the generated content dervied from the previous notebook and use it for analysis.\n",
    "4. **Run Gemini 1.5 Pro with the Prompt:** We execute the prompt with Gemini 1.5 Pro, iterating over 100 samples from the dataset to get classifications for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using This Notebook\n",
    "\n",
    "Colab is recommended for running this notebook, but it can run in any iPython environment where you can connect to Google Cloud, install pip packages, etc.\n",
    "\n",
    "If you're running outside of Colab, depending on your environment you may need to install pip packages (at the very least `pandas` and `tabulate`) that are included in the Colab environment by default but are not part of the Python Standard Library--try pipping the library name of any imports that fail. You'll also notice some comments in code cells that look like \"@something\"; these have special rendering in colab, but you aren't missing out on any content or important functionality.\n",
    "\n",
    "This notebook uses the following Google Cloud services and resources:\n",
    "\n",
    "* [Vertex AI Gemini Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models)\n",
    "* [Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments)\n",
    "* [Google Cloud BigQuery](https://cloud.google.com/bigquery/docs/introduction)\n",
    "\n",
    "This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.12.3\n",
    "* [google-cloud-aiplatform](https://pypi.org/project/google-cloud-aiplatform/) version = 1.70.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Tips\n",
    "\n",
    "1. This notebook uses Generative AI cababilities. Re-running a cell that uses Generative AI capabilities may produce similar but not identical results.\n",
    "2. Please note that running this notebook before running ```01 - LLM Classification Transients.ipynb``` is only possible if you have the following datasets:\n",
    "    * ```data/predictions_results.csv```: The predictions generated with LLMs following instructions in the notebook: ```01 - LLM Classification Transients.ipynb```\n",
    "    * ```data/new_data.npy```: The raw images file. \n",
    "    * ```data/new_labels.csv```: The labels associated with the raw images.\n",
    "\n",
    "TODO: Add tips regarding: Quotas, possible errors (429 etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages and restart runtime \n",
    "Install the required packages for code to run. These packages are included in the requirements.txt file.\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step.\n",
    "\n",
    "The code assumes that the packages are already installed hence the following code is commented out. If you are running this notebook for the first time, please uncomment the next block and run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt --quiet\n",
    "\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate your notebook environment\n",
    "\n",
    "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
    "\n",
    "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
    "\n",
    "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab authentication\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    print('Authenticated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud environment variables information and initialize Clients\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and the following:\n",
    "* [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
    "* [Enable the Big Query API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables will be used throughout the code below. Please find the meaning of each variable in the documentation below:\n",
    "\n",
    "* [PROJECT_ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects): The ID of the GCP project which will host the \n",
    "* [LOCATION](https://cloud.google.com/about/locations): The location where the computational resources will be run. Please also check the availability of Gemini models in the associated regions from [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations).\n",
    "* [EXPERIMENT_NAME](https://cloud.google.com/vertex-ai/docs/experiments/create-manage-exp-run): The name of the Vertex AI experiment. This will ensure tracebility of the experiement in the Vertex AI Environment\n",
    "* [DATASET_ID](https://cloud.google.com/bigquery/docs/datasets-intro): The code will run a batch process for various LLM calls. The results and the input of these runs will be stored at a BigQuery table. The instructions to create a dataset can be found [here](https://cloud.google.com/bigquery/docs/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-69c9c246-e856-4f40-b59f-0b3beac18be9\" href=\"#view-view-vertex-resource-69c9c246-e856-4f40-b59f-0b3beac18be9\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-69c9c246-e856-4f40-b59f-0b3beac18be9');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROJECT_ID = \"<YOUR_PROJECT_ID>\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"supernovadetection\" # @param {type:\"string\"}\n",
    "# Make sure that dataset is created in Big Query\n",
    "DATASET_ID = \"spacehack\" # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initiate Vertex AI and BigQuery clients\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries that will be used throughout the code. Please note that the last import (i.e. helper_functions) is coming originated from the source code in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from helper_functions import write_request, create_batch_prediction_job, batch_data_create, if_tbl_exists, create_ex, save_picture, save_prompt, build_experiment_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build System Instructions and The Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = \"\"\"<PERSONA>\n",
    "You are an experienced astrophysicist tasked with evaluating the accuracy and coherence of astronomical classifications generated by a previous model. Your expertise ensures reliable judgments on how well the output aligns with the given astronomical images.\n",
    "</PERSONA>\"\"\"\n",
    "\n",
    "TASK = \"\"\"<TASK>\n",
    "Your task is to assess the coherence between the provided three images (New, Reference, and Difference) and the classification and description generated by a previous model. Additionally, you will verify if the assigned interest score is appropriate based on the description and images.\n",
    "</TASK>\\n\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTIONS = \"\"\"<INSTRUCTIONS>\n",
    "**1. Coherence Evaluation**\n",
    "- Review the classification and description given by the previous model.\n",
    "- Judge how well the model’s output matches the observed features in the images.\n",
    "- Assign a coherence score from 0 to 5:\n",
    "  - **5** - Perfectly coherent\n",
    "  - **4** - Almost entirely correct\n",
    "  - **3** - Mostly correct with some errors\n",
    "  - **2** - More incorrect than correct\n",
    "  - **1** - Majority incorrect\n",
    "  - **0** - Complete hallucination\n",
    "\n",
    "**2. Interest Score Validation**\n",
    "- Determine if the interest score given by the model is validated with the images and description.\n",
    "- Respond with a simple **Yes** (validated) or **No** (invalidated).\n",
    "</INSTRUCTIONS>\"\"\"\n",
    "\n",
    "\n",
    "METHOD = \"\"\"<METHOD>\n",
    "1. Examine the images and the model’s classification and description.\n",
    "2. Judge coherence, assign a score (0-5), and note any major discrepancies.\n",
    "3. Validate if the interest score is consistent with the description and images, responding with Yes or No.\n",
    "</METHOD>\n",
    "\"\"\"\n",
    "\n",
    "# Collapse the System Instructions into a single variable\n",
    "stat_prompt = PERSONA + TASK + INSTRUCTIONS + METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of images\n",
    "file_path_data = 'data/new_data.npy'\n",
    "file_path_labels_csv = 'data/new_labels.csv'\n",
    "predictions_file = 'data/predictions_results.csv'\n",
    "\n",
    "# Load image triplets (New, Reference, Difference)\n",
    "triplets = np.load(file_path_data)\n",
    "\n",
    "# Load labels and predictions from CSV files\n",
    "labels_df = pd.read_csv(file_path_labels_csv)\n",
    "predictions_df = pd.read_csv(predictions_file)\n",
    "\n",
    "# New examples: 1 TN, 1TP, 2FP, 2FN\n",
    "sample_indexes = [2065, 294, 454 , 216, 2284, 2055]\n",
    "for i in sample_indexes:\n",
    "    save_picture(triplets, i, True)\n",
    "\n",
    "valid_indexes = np.where(~np.isnan(triplets).any(axis=(1, 2, 3)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated descriptions for the new task format\n",
    "def valid_example_gen(index_no):\n",
    "  return OrderedDict({\n",
    "    \"Actual\": predictions_df.label[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction\": predictions_df.predicted[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction_Explanation\": predictions_df.explanation[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Other_LLM_Interest_score\": predictions_df.interest_score[predictions_df.index_no==index_no].iloc[0]    \n",
    "  })\n",
    "## DESCRIPTION INDEX 181:\n",
    "desc1_old = valid_example_gen(181)\n",
    "desc1_new = {\"coherence_score\": 1, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 294\n",
    "desc2_old = valid_example_gen(294)\n",
    "desc2_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 454:\n",
    "desc3_old = valid_example_gen(454)\n",
    "desc3_new = {\"coherence_score\": 0, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 2065:\n",
    "desc4_old = valid_example_gen(2065)\n",
    "desc4_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 216:\n",
    "desc5_old = valid_example_gen(216)\n",
    "desc5_new = {\"coherence_score\": 2, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 448:\n",
    "desc6_old = valid_example_gen(448)\n",
    "desc6_new = {\"coherence_score\": 4, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "descriptions = [\n",
    "    desc1_old, desc1_new, desc2_old, desc2_new, desc3_old, desc3_new, desc4_old, desc4_new, desc5_old, desc5_new, desc6_old, desc6_new,   \n",
    "]\n",
    "### Write the examples used in a readable format to be saved as a txt file for tracebility\n",
    "example_description = list(zip([\"DESCRIPTION INDEX: \" + str(x) for x in sample_indexes], descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 6 examples for the dynamic prompt\n",
    "EXAMPLES = [\"<EXAMPLES>\\n\"]\n",
    "for i in range(len(sample_indexes)):\n",
    "    \n",
    "    str_EX = f\"\"\"Example {i+1}:\n",
    "    \"\"\"\n",
    "    all_list = create_ex(sample_indexes[i], True)\n",
    "    all_list.insert(0, str_EX)\n",
    "    all_list.append(str(dict(descriptions[2*i])))\n",
    "    all_list.append(str(dict(descriptions[2*i+1])))\n",
    "    \n",
    "    for k in all_list:\n",
    "        EXAMPLES.append(k)\n",
    "EXAMPLES.append(\"\\n</EXAMPLES>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/355771430623/locations/us-central1/metadataStores/default/contexts/supernovadetection-run202503210617 to Experiment: supernovadetection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-a3581089-9dbf-45eb-8bec-5f306c2a5d05\" href=\"#view-view-vertex-resource-a3581089-9dbf-45eb-8bec-5f306c2a5d05\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-a3581089-9dbf-45eb-8bec-5f306c2a5d05');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs/supernovadetection-run202503210617?project=turan-genai-bb');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs/supernovadetection-run202503210617?project=turan-genai-bb', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start logging the experiment\n",
    "\n",
    "## Prepare the variables\n",
    "timestamp = datetime.datetime.now()\n",
    "formatted_datetime = timestamp.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "## Log the experiments variables\n",
    "### Create the run name with timestamp\n",
    "run_name = \"run\" + formatted_datetime\n",
    "DESCRIPTION = \"\"\"LLM as a judge run second try\n",
    "\"\"\" # @param {type:\"string\"}\n",
    "MODEL = \"gemini-1.5-pro-002\" # @param [gemini-1.5-pro-001\", \"gemini-1.5-flash-001\", \"gemini-1.0-pro-002\"]\n",
    "TEMPERATURE = 0.1 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "TOP_P = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "PROMPT_FILE = save_prompt(stat_prompt + '\\n'.join([a + \"\\n\" + str(b) + \"\\n\" for (a,b) in example_description]), run_name)\n",
    "\n",
    "# Build the experimentation variables\n",
    "exp_vars = build_experiment_vars(description=DESCRIPTION,prompt=PROMPT_FILE, model=MODEL, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "# Start the run\n",
    "aiplatform.start_run(run_name)\n",
    "# Log the experiment variables\n",
    "aiplatform.log_params(exp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688 current state:\n",
      "4\n",
      "BatchPredictionJob run completed. Resource name: projects/355771430623/locations/us-central1/batchPredictionJobs/8905182734805106688\n"
     ]
    }
   ],
   "source": [
    "# Construct table names\n",
    "input_table_name = f'{PROJECT_ID}.{DATASET_ID}.input{run_name}'\n",
    "output_table_name = f'{PROJECT_ID}.{DATASET_ID}.output{run_name}'\n",
    "\n",
    "# Define the table schema\n",
    "schema = [\n",
    "    bigquery.SchemaField('request', 'JSON'),\n",
    "    bigquery.SchemaField('index_no', 'INTEGER')\n",
    "]\n",
    "\n",
    "# Create the table if it doesnt exist\n",
    "table = bigquery.Table(input_table_name, schema=schema)\n",
    "if_tbl_exists(bq_client, table)\n",
    "\n",
    "# Create the pandas df that stores the requests\n",
    "batch_df = pd.DataFrame(columns=[\"request\", \"index_no\"])\n",
    "\n",
    "for t in list(predictions_df.index_no):\n",
    "    dyna_prompt = EXAMPLES + create_ex(t, False) + [str(dict(valid_example_gen(t)))]\n",
    "    df_temp = pd.DataFrame([[batch_data_create(stat_prompt, dyna_prompt, TEMPERATURE, TOP_P), t]], columns=[\"request\", \"index_no\"])\n",
    "    batch_df = pd.concat([batch_df, df_temp], ignore_index=True)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "job_config.source_format = 'CSV'\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    batch_df, input_table_name, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "# Generate the request.json for batch processing\n",
    "write_request(\"spacehackbatch_check\", MODEL, \"bq://\" + input_table_name,\n",
    "            \"bq://\" + output_table_name)\n",
    "\n",
    "# Send the batch response\n",
    "response = create_batch_prediction_job(PROJECT_ID, \"request.json\")\n",
    "# Run the batch process job and wait for completion.\n",
    "job = aiplatform.BatchPredictionJob(response[\"name\"].split(\"/\")[-1])\n",
    "job.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The query to generate a final table with results\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{run_name}` AS\n",
    "SELECT  t1.index_no, \n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.coherence_score') AS coherence_score,\n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.interest_score_validated') AS interest_score_coherent,\n",
    "t1.response, t1.request \n",
    "        FROM `{output_table_name}` as t1\n",
    "\"\"\"\n",
    "#Run the query\n",
    "query_job = bq_client.query(create_table_query)\n",
    "results = query_job.result()\n",
    "# Download the results to generate KPIs\n",
    "download_query = f\"\"\"\n",
    "SELECT index_no, coherence_score, interest_score_coherent\n",
    "FROM {PROJECT_ID}.{DATASET_ID}.{run_name}\n",
    "\"\"\"\n",
    "pred_df = bq_client.query_and_wait(download_query).to_dataframe()\n",
    "pred_df.to_csv(\"data/predictions_with_Coherence.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Further analysis on coherence_score and interest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the resources. \n",
    "\n",
    "# Delete the BigQeury Tables\n",
    "bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.input{run_name}\")\n",
    "bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.output{run_name}\")\n",
    "bq_client.delete_table(f\"{PROJECT_ID}.{DATASET_ID}.{run_name}\")\n",
    "\n",
    "# Delte the request.json file\n",
    "os.remove(\"request.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_no</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>interest_score_coherent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>553</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>859</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>2466</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>1129</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>3071</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>866</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3219 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index_no coherence_score interest_score_coherent\n",
       "0          817               5                     Yes\n",
       "1          750               5                     Yes\n",
       "2          553               5                     Yes\n",
       "3          101               5                     Yes\n",
       "4          859               5                     Yes\n",
       "...        ...             ...                     ...\n",
       "3214       800               5                     Yes\n",
       "3215      2466               5                     Yes\n",
       "3216      1129               5                     Yes\n",
       "3217      3071               5                     Yes\n",
       "3218       866               5                     Yes\n",
       "\n",
       "[3219 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
