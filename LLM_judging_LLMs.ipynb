{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"turan-genai-bb\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"supernovadetection\" # @param {type:\"string\"}\n",
    "# Make sure that dataset is created in Big Query\n",
    "DATASET_ID = \"spacehack\" # @param {type:\"string\"}\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "import base64\n",
    "import json\n",
    "import random, os\n",
    "from collections import OrderedDict\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason, Image\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from helper_functions import batch_data_create, build_run_batch, if_tbl_exists, create_ex, save_picture, save_prompt, build_experiment_vars, create_batch_prediction_job, write_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = \"\"\"<PERSONA>\n",
    "You are an experienced astrophysicist tasked with evaluating the accuracy and coherence of astronomical classifications generated by a previous model. Your expertise ensures reliable judgments on how well the output aligns with the given astronomical images.\n",
    "</PERSONA>\"\"\"\n",
    "\n",
    "TASK = \"\"\"<TASK>\n",
    "Your task is to assess the coherence between the provided three images (New, Reference, and Difference) and the classification and description generated by a previous model. Additionally, you will verify if the assigned interest score is appropriate based on the description and images.\n",
    "</TASK>\\n\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTIONS = \"\"\"<INSTRUCTIONS>\n",
    "**1. Coherence Evaluation**\n",
    "- Review the classification and description given by the previous model.\n",
    "- Judge how well the model’s output matches the observed features in the images.\n",
    "- Assign a coherence score from 0 to 5:\n",
    "  - **5** - Perfectly coherent\n",
    "  - **4** - Almost entirely correct\n",
    "  - **3** - Mostly correct with some errors\n",
    "  - **2** - More incorrect than correct\n",
    "  - **1** - Majority incorrect\n",
    "  - **0** - Complete hallucination\n",
    "\n",
    "**2. Interest Score Validation**\n",
    "- Determine if the interest score given by the model is validated with the images and description.\n",
    "- Respond with a simple **Yes** (validated) or **No** (invalidated).\n",
    "</INSTRUCTIONS>\"\"\"\n",
    "\n",
    "\n",
    "METHOD = \"\"\"<METHOD>\n",
    "1. Examine the images and the model’s classification and description.\n",
    "2. Judge coherence, assign a score (0-5), and note any major discrepancies.\n",
    "3. Validate if the interest score is consistent with the description and images, responding with Yes or No.\n",
    "</METHOD>\n",
    "\"\"\"\n",
    "\n",
    "# Collapse the System Instructions into a single variable\n",
    "stat_prompt = PERSONA + TASK + INSTRUCTIONS + METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of images\n",
    "file_path_data = 'data/new_data.npy'\n",
    "file_path_labels_csv = 'data/new_labels.csv'\n",
    "predictions_file = 'data/MeerLICHT_predictions.csv'\n",
    "\n",
    "# Load image triplets (New, Reference, Difference)\n",
    "triplets = np.load(file_path_data)\n",
    "\n",
    "# Load labels and predictions from CSV files\n",
    "labels_df = pd.read_csv(file_path_labels_csv)\n",
    "# Download the predictions file then read it\n",
    "id =  \"1e4TNV5evBGdnerB0K1wOIpFGHvpIWZJL\" #id from the Google Drive\n",
    "gdown.download(id=id, output = predictions_file)\n",
    "predictions_df = pd.read_csv(predictions_file)\n",
    "\n",
    "# Sample indexes for saving example images\n",
    "# sample_indexes = [0, 1, 3, 4, 8, 48, 77, 1179, 1180, 1181, 1191, 1193, 592, 685, 3216]\n",
    "# New examples: 1 TN, 1TP, 2FP, 2FN\n",
    "sample_indexes = [2259, 294, 1088 , 2065, 631, 448]\n",
    "for i in sample_indexes:\n",
    "    save_picture(triplets, i, True)\n",
    "\n",
    "valid_indexes = np.where(~np.isnan(triplets).any(axis=(1, 2, 3)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated descriptions for the new task format\n",
    "def valid_example_gen(index_no):\n",
    "  return OrderedDict({\n",
    "    \"Actual\": predictions_df.actual[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction\": predictions_df.predicted[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction_Explanation\": predictions_df.explanation[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Other_LLM_Interest_score\": predictions_df.interest_score[predictions_df.index_no==index_no].iloc[0]    \n",
    "  })\n",
    "## DESCRIPTION INDEX 2259:\n",
    "desc1_old = valid_example_gen(2259)\n",
    "desc1_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "\n",
    "## DESCRIPTION INDEX 294\n",
    "desc2_old = valid_example_gen(294)\n",
    "desc2_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 1088:\n",
    "desc3_old = valid_example_gen(1088)\n",
    "desc3_new = {\"coherence_score\": 3, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 2065:\n",
    "desc4_old = valid_example_gen(2065)\n",
    "desc4_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 631:\n",
    "desc5_old = valid_example_gen(631)\n",
    "desc5_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 448:\n",
    "desc6_old = valid_example_gen(448)\n",
    "desc6_new = {\"coherence_score\": 4, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "descriptions = [\n",
    "    desc1_old, desc1_new, desc2_old, desc2_new, desc3_old, desc3_new, desc4_old, desc4_new, desc5_old, desc5_new, desc6_old, desc6_new,   \n",
    "]\n",
    "### Write the examples used in a readable format to be saved as a txt file for tracebility\n",
    "example_description = list(zip([\"DESCRIPTION INDEX: \" + str(x) for x in sample_indexes], descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 6 examples for the dynamic prompt\n",
    "EXAMPLES = [\"<EXAMPLES>\\n\"]\n",
    "for i in range(len(sample_indexes)):\n",
    "    \n",
    "    str_EX = f\"\"\"Example {i+1}:\n",
    "    \"\"\"\n",
    "    all_list = create_ex(sample_indexes[i], True)\n",
    "    all_list.insert(0, str_EX)\n",
    "    all_list.append(str(dict(descriptions[2*i])))\n",
    "    all_list.append(str(dict(descriptions[2*i+1])))\n",
    "    \n",
    "    for k in all_list:\n",
    "        EXAMPLES.append(k)\n",
    "EXAMPLES.append(\"\\n</EXAMPLES>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging the experiment\n",
    "\n",
    "## Prepare the variables\n",
    "timestamp = datetime.datetime.now()\n",
    "formatted_datetime = timestamp.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "## Log the experiments variables\n",
    "### Create the run name with timestamp\n",
    "run_name = \"run\" + formatted_datetime\n",
    "DESCRIPTION = \"\"\"LLM as a judge run\n",
    "\"\"\" # @param {type:\"string\"}\n",
    "MODEL = \"gemini-1.5-pro-002\" # @param [gemini-1.5-pro-001\", \"gemini-1.5-flash-001\", \"gemini-1.0-pro-002\"]\n",
    "TEMPERATURE = 0.1 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "TOP_P = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "PROMPT_FILE = save_prompt(stat_prompt + '\\n'.join([a + \"\\n\" + str(b) + \"\\n\" for (a,b) in example_description]), run_name)\n",
    "\n",
    "# Build the experimentation variables\n",
    "exp_vars = build_experiment_vars(description=DESCRIPTION,prompt=PROMPT_FILE, model=MODEL, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "# Start the run\n",
    "aiplatform.start_run(run_name)\n",
    "# Log the experiment variables\n",
    "aiplatform.log_params(exp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct table names\n",
    "input_table_name = f'{PROJECT_ID}.{DATASET_ID}.input{run_name}'\n",
    "output_table_name = f'{PROJECT_ID}.{DATASET_ID}.output{run_name}'\n",
    "\n",
    "# Define the table schema\n",
    "schema = [\n",
    "    bigquery.SchemaField('request', 'JSON'),\n",
    "    bigquery.SchemaField('index_no', 'INTEGER')\n",
    "]\n",
    "\n",
    "# Create the table if it doesnt exist\n",
    "table = bigquery.Table(input_table_name, schema=schema)\n",
    "if_tbl_exists(bq_client, table)\n",
    "\n",
    "# Create the pandas df that stores the requests\n",
    "batch_df = pd.DataFrame(columns=[\"request\", \"index_no\"])\n",
    "\n",
    "for t in list(predictions_df.index_no):\n",
    "    dyna_prompt = EXAMPLES + create_ex(t, False) + [str(dict(valid_example_gen(t)))]\n",
    "    df_temp = pd.DataFrame([[batch_data_create(stat_prompt, dyna_prompt, TEMPERATURE, TOP_P), t]], columns=[\"request\", \"index_no\"])\n",
    "    batch_df = pd.concat([batch_df, df_temp], ignore_index=True)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "job_config.source_format = 'CSV'\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    batch_df, input_table_name, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "# Generate the request.json for batch processing\n",
    "write_request(\"spacehackbatch_check\", MODEL, \"bq://\" + input_table_name,\n",
    "            \"bq://\" + output_table_name)\n",
    "\n",
    "# Send the batch response\n",
    "response = create_batch_prediction_job(PROJECT_ID, \"request.json\")\n",
    "# Run the batch process job and wait for completion.\n",
    "job = aiplatform.BatchPredictionJob(response[\"name\"].split(\"/\")[-1])\n",
    "job.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query to generate a final table with results\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{run_name}` AS\n",
    "SELECT  t1.index_no, \n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.coherence_score') AS coherence_score,\n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.interest_score_validated') AS interest_score_coherent,\n",
    "t1.response, t1.request \n",
    "        FROM `{output_table_name}` as t1\n",
    "\"\"\"\n",
    "# Run the query\n",
    "query_job = bq_client.query(create_table_query)\n",
    "results = query_job.result()\n",
    "# Download the results to generate KPIs\n",
    "download_query = f\"\"\"\n",
    "SELECT index_no, coherence_score, interest_score_coherent\n",
    "FROM {PROJECT_ID}.{DATASET_ID}.{run_name} \n",
    "\"\"\"\n",
    "pred_df = bq_client.query_and_wait(download_query).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.merge(predictions_df, on=\"index_no\").to_csv(\"MeerLICHT_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
