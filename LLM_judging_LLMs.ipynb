{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/spacehack/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7b6d0a2c-c963-4e19-a10c-b86aa5aff8f9\" href=\"#view-view-vertex-resource-7b6d0a2c-c963-4e19-a10c-b86aa5aff8f9\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7b6d0a2c-c963-4e19-a10c-b86aa5aff8f9');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs?project=turan-genai-bb', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/spacehack/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"turan-genai-bb\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"supernovadetection\" # @param {type:\"string\"}\n",
    "# Make sure that dataset is created in Big Query\n",
    "DATASET_ID = \"spacehack\" # @param {type:\"string\"}\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "import base64\n",
    "import json\n",
    "import random, os\n",
    "from collections import OrderedDict\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason, Image\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from helper_functions import batch_data_create, build_run_batch, if_tbl_exists, create_ex, save_picture, save_prompt, build_experiment_vars, create_batch_prediction_job, write_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSONA = \"\"\"<PERSONA>\n",
    "# You are an experienced astrophysicist tasked with evaluating the accuracy and coherence of astronomical classifications generated by a previous model. Your expertise ensures reliable judgments on how well the output aligns with the given astronomical images.\n",
    "# </PERSONA>\"\"\"\n",
    "\n",
    "# TASK = \"\"\"<TASK>\n",
    "# Your task is to assess the coherence between the provided three images (New, Reference, and Difference) and the classification and description generated by a previous model. Additionally, you will verify if the assigned interest score is appropriate based on the description and images.\n",
    "# </TASK>\\n\n",
    "# \"\"\"\n",
    "# INSTRUCTIONS = \"\"\"<INSTRUCTIONS>\n",
    "# **1. Coherence Evaluation**\n",
    "# - **Review Process:**\n",
    "#   - Examine the classification and description provided by the previous model.\n",
    "#   - Analyze how accurately the model’s output reflects the features observed in the New, Reference, and Difference images.\n",
    "\n",
    "# - **Scoring Criteria:**\n",
    "#   - **5 – Completely Coherent and Accurate**\n",
    "#     - The classification and description perfectly match all relevant features in the images.\n",
    "#     - No discrepancies or inaccuracies are present.\n",
    "#     - The explanation is thorough and leaves no room for doubt regarding its correctness.\n",
    "\n",
    "#   - **4 – Largely Correct with Minor Discrepancies**\n",
    "#     - The classification and description are mostly accurate and align well with the images.\n",
    "#     - Minor errors or omissions exist but do not significantly detract from the overall accuracy.\n",
    "#     - These discrepancies are typically small details that do not alter the fundamental understanding.\n",
    "\n",
    "#   - **3 – Generally Correct but with Notable Errors**\n",
    "#     - The classification and description capture the primary features but include several significant inaccuracies.\n",
    "#     - These errors may lead to a misunderstanding of key aspects depicted in the images.\n",
    "#     - The overall description remains somewhat reliable but requires corrections for precision.\n",
    "\n",
    "#   - **2 – More Inaccuracies Than Correct Elements**\n",
    "#     - The classification and description contain numerous errors that overshadow the correct information.\n",
    "#     - Critical features are misrepresented, leading to a largely incorrect understanding of the images.\n",
    "#     - The output may include irrelevant or misleading information not supported by the images.\n",
    "\n",
    "#   - **1 – Predominantly Incorrect**\n",
    "#     - The classification and description are mostly inaccurate with very few correct elements.\n",
    "#     - There is a fundamental misunderstanding of the features presented in the images.\n",
    "#     - The output fails to convey the essential characteristics necessary for accurate classification.\n",
    "\n",
    "#   - **0 – Entirely Fabricated or Unrelated**\n",
    "#     - The classification and description have no basis in the provided images.\n",
    "#     - The output is completely irrelevant, containing information that does not pertain to the images.\n",
    "#     - It may include fabricated details with no observable support from the visual data.\n",
    "\n",
    "# **2. Interest Score Validation**\n",
    "# - **Evaluation Process:**\n",
    "#   - Assess whether the interest score assigned by the model is justified based on the description and the provided images.\n",
    "#   - **Self-Consistency Check:** Ensure that the interest score is coherent with the model’s own description, even in cases where the description may contain inaccuracies. This means evaluating if the model consistently assigns an appropriate interest score relative to the content and significance it describes, maintaining internal consistency.\n",
    "\n",
    "# - **Response Format:**\n",
    "#   - Respond with a clear **Yes** (validated) or **No** (invalidated).\n",
    "# </INSTRUCTIONS>\"\"\"\n",
    "\n",
    "# METHOD = \"\"\"<METHOD>\n",
    "# 1. Examine the images and the model’s classification and description.\n",
    "# 2. Judge coherence, assign a score (0-5), and note any major discrepancies.\n",
    "# 3. Validate if the interest score is consistent with the description and images, responding with Yes or No.\n",
    "# </METHOD>\n",
    "# \"\"\"\n",
    "\n",
    "# # Collapse the System Instructions into a single variable\n",
    "# stat_prompt = PERSONA + TASK + INSTRUCTIONS + METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = \"\"\"<PERSONA>\n",
    "You are an experienced astrophysicist tasked with evaluating the accuracy, coherence, and adherence to task requirements of astronomical classifications generated by a previous model. Your expertise ensures reliable judgments on how well the output aligns with the given astronomical images and the original instructions for the task.\n",
    "</PERSONA>\"\"\"\n",
    "\n",
    "TASK = \"\"\"<TASK>\n",
    "Your task is to assess the coherence between the provided three images (New, Reference, and Difference) and the classification and description generated by a previous model. Additionally, evaluate how well the output adheres to the original instructions provided to the model. Verify if the assigned interest score is appropriate based on the description, images, and original instructions.\n",
    "</TASK>\"\"\"\n",
    "\n",
    "ORIGINAL_INSTRUCTIONS = \"\"\"<ORIGINAL INSTRUCTIONS>\n",
    "**Persona**  \n",
    "You are an experienced astrophysicist, and your task is to classify astronomical transients into Real or Bogus based on a given set of 3 images. You have seen thousands of astronomical images during your lifetime and you are very good at making this classification by looking at the images and following the instructions.\n",
    "\n",
    "**Task**  \n",
    "Your task is to read the INSTRUCTIONS, look at the 3 images (New, Reference, and Difference images) and classify if the source at the center of the cutout and inside the red circle is a Real or Bogus astronomical transient. Provide your thought process to explain how you reasoned to provide the response. Respond in JSON format.\n",
    "\n",
    "**Instructions**  \n",
    "1. **Purpose**:  \n",
    "   Help vet astronomical data for the Real/Bogus classification. The goal is for you to use your expertise to distinguish between real and bogus sources.\n",
    "\n",
    "2. **Information Provided**:  \n",
    "   You will be shown three astronomical image cutouts:  \n",
    "   a) **New Image:** The newest image centered at the location of the suspected transient source.  \n",
    "   b) **Reference Image:** A reference image from the same telescope of the same part of the sky to be used for comparison. It shows if the source was already there in the past or not.  \n",
    "   c) **Difference Image:** The residual image after the new and reference images are subtracted. Real sources should appear in this cutout as circular objects with only positive (white pixels) or only negative (black pixels) flux.\n",
    "\n",
    "3. **Criteria for Classification**:  \n",
    "   - **Real Source:**  \n",
    "     - **Shape:** Circular shape at the center of the cutout with a visual extent of ~5-10 pixels, varying with focus conditions.  \n",
    "     - **Brightness:** Positive flux (white pixels) in either the new or reference image. Positive or negative flux in the Difference image.  \n",
    "     - **Variability:** The source at the center can fade or brighten between the new and reference images, appearing as positive or negative in the Difference image.  \n",
    "     - **Presence:** The source may (dis)appear between the new and reference images. A source may also appear on top of an underlying source (e.g., supernova on a galaxy).  \n",
    "\n",
    "   - **Bogus Source:**  \n",
    "     - **Shape:** Non-circular shape (e.g., elongated). This includes irregular shapes, positive or negative, like streaks or lines caused by cosmic-rays, diffraction spikes, and cross-talk.  \n",
    "     - **Brightness:** Negative flux (black pixels) at the center of the cutout in either the new or reference image. The source at the center can never be negative in the New or Reference image, only in the Difference.  \n",
    "     - **Misalignment:** If the source in the New and Reference images is misaligned, it will show a Yin-Yang pattern (both white and black) in the Difference image.\n",
    "\n",
    "4. **Additional Guidance**:  \n",
    "   - **Contextual Information:** Focus on the source at the center of the cutouts inside the red circle, but consider nearby sources to diagnose potential problems.  \n",
    "   - **Examples:** Refer to provided visual examples of real and bogus sources to aid in identification.  \n",
    "   - **Judgment Criteria:** For ambiguous cases or borderline scenarios, consider the overall context and consistency with known characteristics of real and bogus sources.\n",
    "\n",
    "**Method**  \n",
    "1. **Focus on the Red Circle**: Start by examining the source located at the center of the cutout and inside the red circle.  \n",
    "2. **Analyze Each Image Individually**:  \n",
    "   - **New Image**: Check for the presence, shape, and brightness of the source in the new image.  \n",
    "   - **Reference Image**: Compare the source's properties in the reference image to those in the new image.  \n",
    "   - **Difference Image**: Observe the residuals that result from subtracting the reference image from the new image. Look for patterns (circular, positive/negative flux) that match characteristics of Real or Bogus sources.  \n",
    "3. **Evaluate Features**:  \n",
    "   - Examine the shape, brightness, and other relevant features (e.g., artifacts, misalignments) of the source in each image.  \n",
    "   - Determine if these features are consistent with a Real or Bogus classification based on the criteria provided in the instructions.  \n",
    "4. **Consider Relationships Between Images**:  \n",
    "   - Compare the new, reference, and difference images to understand any changes in the source over time.  \n",
    "   - Look for discrepancies or confirmations that might support or contradict a particular classification.  \n",
    "5. **Employ a Chain-of-Thought Reasoning**:  \n",
    "   - Clearly outline each observation you make and explain how it contributes to your decision-making process.  \n",
    "   - If you find any contradictions or ambiguous features, acknowledge them and provide reasoning for your final decision.  \n",
    "6. **Assign an Interest Score**:  \n",
    "   - After determining if the source is Real or Bogus, assign an appropriate interest score:  \n",
    "     - 'No interest' for Bogus sources.  \n",
    "     - 'Low interest' for variable transients.  \n",
    "     - 'High interest' for explosive transients.  \n",
    "7. **Prepare the Final Output in JSON Format**:  \n",
    "   - Format your response as a JSON object containing:  \n",
    "     - The classification ('Real' or 'Bogus').  \n",
    "     - An explanation detailing your thought process and observations.  \n",
    "     - The assigned interest score.  \n",
    "\n",
    "8. **Example Output**:  \n",
    "   - Refer to the provided examples to see the expected format and detail level of your response.\n",
    "</ORIGINAL INSTRUCTIONS>\"\"\"\n",
    "\n",
    "INSTRUCTIONS = \"\"\"<INSTRUCTIONS>\n",
    "**1. Coherence and Adherence Evaluation**  \n",
    "- **Review Process**:  \n",
    "  - Examine the classification and description provided by the previous model.  \n",
    "  - Analyze how accurately the model’s output reflects the features observed in the New, Reference, and Difference images.  \n",
    "  - Compare the output against the original instructions to assess adherence to task requirements.  \n",
    "\n",
    "- **Scoring Criteria**:  \n",
    "  - **5 – Completely Coherent, Accurate, and Adherent**  \n",
    "    - The classification and description perfectly match all relevant features in the images and fully adhere to the original instructions.  \n",
    "  - **4 – Largely Correct with Minor Issues**  \n",
    "    - The output is mostly accurate and aligns well with both the images and instructions, with only minor errors or deviations.  \n",
    "  - **3 – Generally Correct but with Notable Errors**  \n",
    "    - The output captures primary features but includes significant inaccuracies or partial adherence to the instructions.  \n",
    "  - **2 – More Inaccuracies Than Correct Elements**  \n",
    "    - The output has numerous errors, misrepresenting key features and deviating from the instructions.  \n",
    "  - **1 – Predominantly Incorrect**  \n",
    "    - The output is mostly inaccurate and fails to follow the instructions in key areas.  \n",
    "  - **0 – Entirely Fabricated or Unrelated**  \n",
    "    - The output has no basis in the images or instructions.  \n",
    "\n",
    "**2. Interest Score Validation**  \n",
    "- Validate whether the interest score is consistent with both the description and the images, as well as the original instructions.\n",
    "\n",
    "**3. Response Format**  \n",
    "- Provide a JSON response with:  \n",
    "  - **Coherence and adherence score (0-5).**  \n",
    "  - **Validation of the interest score (Yes/No).**  \n",
    "  - **Detailed reasoning** for the given score and validation decision.\n",
    "</INSTRUCTIONS>\"\"\"\n",
    "\n",
    "METHOD = \"\"\"<METHOD>\n",
    "1. Examine the images and the model’s classification and description.  \n",
    "2. Compare the output against the original instructions for adherence.  \n",
    "3. Judge coherence, assign a score (0-5), and note any major discrepancies.  \n",
    "4. Validate if the interest score is consistent with the description, images, and instructions, responding with Yes or No.  \n",
    "5. Provide detailed reasoning for all evaluations.\n",
    "</METHOD>\"\"\"\n",
    "\n",
    "stat_prompt = PERSONA + TASK + ORIGINAL_INSTRUCTIONS + INSTRUCTIONS + METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1e4TNV5evBGdnerB0K1wOIpFGHvpIWZJL\n",
      "To: /home/user/spacehack/MeerLICHT_predictions.csv\n",
      "100%|██████████| 1.04M/1.04M [00:00<00:00, 92.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset of images\n",
    "file_path_data = 'MeerLICHT_dataset/MeerLICHT_images.npy'\n",
    "file_path_labels_csv = 'MeerLICHT_dataset/MeerLICHT_labels.csv'\n",
    "predictions_file = 'MeerLICHT_predictions.csv'\n",
    "\n",
    "# Load image triplets (New, Reference, Difference)\n",
    "triplets = np.load(file_path_data)\n",
    "\n",
    "# Load labels and predictions from CSV files\n",
    "labels_df = pd.read_csv(file_path_labels_csv)\n",
    "# Download the predictions file then read it\n",
    "id =  \"1e4TNV5evBGdnerB0K1wOIpFGHvpIWZJL\" #id from the Google Drive\n",
    "gdown.download(id=id, output = predictions_file)\n",
    "predictions_df = pd.read_csv(predictions_file)\n",
    "\n",
    "# Sample indexes for saving example images\n",
    "# New examples: 1 TN, 1TP, 2FP, 2FN\n",
    "sample_indexes = [181, 294, 454 , 2065, 216, 448,624]\n",
    "# for i in sample_indexes:\n",
    "#     save_picture(triplets, i, True)\n",
    "\n",
    "valid_indexes = np.where(~np.isnan(triplets).any(axis=(1, 2, 3)))[0]\n",
    "# for t in valid_indexes:\n",
    "#     save_picture(triplets, t, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated descriptions for the new task format\n",
    "def valid_example_gen(index_no):\n",
    "  return OrderedDict({\n",
    "    \"Actual\": predictions_df.actual[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction\": predictions_df.predicted[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Prediction_Explanation\": predictions_df.explanation[predictions_df.index_no==index_no].iloc[0],\n",
    "    \"Other_LLM_Interest_score\": predictions_df.interest_score[predictions_df.index_no==index_no].iloc[0]    \n",
    "  })\n",
    "\n",
    "## DESCRIPTION INDEX 181:\n",
    "desc1_old = valid_example_gen(181)\n",
    "desc1_new = {\"coherence_score\": 2, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 294\n",
    "desc2_old = valid_example_gen(294)\n",
    "desc2_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 454:\n",
    "desc3_old = valid_example_gen(454)\n",
    "desc3_new = {\"coherence_score\": 1, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 2065:\n",
    "desc4_old = valid_example_gen(2065)\n",
    "desc4_new = {\"coherence_score\": 5, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 216:\n",
    "desc5_old = valid_example_gen(216)\n",
    "desc5_new = {\"coherence_score\": 2, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 448:\n",
    "desc6_old = valid_example_gen(448)\n",
    "desc6_new = {\"coherence_score\": 4, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "## DESCRIPTION INDEX 624:\n",
    "desc7_old = valid_example_gen(624)\n",
    "desc7_new = {\"coherence_score\": 3, \"interest_score_validated\": \"Yes\"}\n",
    "\n",
    "descriptions = [\n",
    "    desc1_old, desc1_new, desc2_old, desc2_new, desc3_old, desc3_new, desc4_old, desc4_new, desc5_old, desc5_new, desc6_old, desc6_new,  desc7_old, desc7_new, \n",
    "]\n",
    "### Write the examples used in a readable format to be saved as a txt file for tracebility\n",
    "example_description = list(zip([\"DESCRIPTION INDEX: \" + str(x) for x in sample_indexes], descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 6 examples for the dynamic prompt\n",
    "EXAMPLES = [\"<EXAMPLES>\\n\"]\n",
    "for i in range(len(sample_indexes)):\n",
    "    \n",
    "    str_EX = f\"\"\"Example {i+1}:\n",
    "    \"\"\"\n",
    "    all_list = create_ex(sample_indexes[i], True)\n",
    "    all_list.insert(0, str_EX)\n",
    "    all_list.append(str(dict(descriptions[2*i])))\n",
    "    all_list.append(str(dict(descriptions[2*i+1])))\n",
    "    \n",
    "    for k in all_list:\n",
    "        EXAMPLES.append(k)\n",
    "EXAMPLES.append(\"\\n</EXAMPLES>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/355771430623/locations/us-central1/metadataStores/default/contexts/supernovadetection-run202412150005 to Experiment: supernovadetection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-751845de-c7eb-41fe-8740-21722ef3c812\" href=\"#view-view-vertex-resource-751845de-c7eb-41fe-8740-21722ef3c812\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-751845de-c7eb-41fe-8740-21722ef3c812');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs/supernovadetection-run202412150005?project=turan-genai-bb');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/supernovadetection/runs/supernovadetection-run202412150005?project=turan-genai-bb', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start logging the experiment\n",
    "\n",
    "## Prepare the variables\n",
    "timestamp = datetime.datetime.now()\n",
    "formatted_datetime = timestamp.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "## Log the experiments variables\n",
    "### Create the run name with timestamp\n",
    "run_name = \"run\" + formatted_datetime\n",
    "DESCRIPTION = \"\"\"LLM as a judge run second try\n",
    "\"\"\" # @param {type:\"string\"}\n",
    "MODEL = \"gemini-1.5-pro-002\" # @param [gemini-1.5-pro-001\", \"gemini-1.5-flash-001\", \"gemini-1.0-pro-002\"]\n",
    "TEMPERATURE = 0.1 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "TOP_P = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "PROMPT_FILE = save_prompt(stat_prompt + '\\n'.join([a + \"\\n\" + str(b) + \"\\n\" for (a,b) in example_description]), run_name)\n",
    "\n",
    "# Build the experimentation variables\n",
    "exp_vars = build_experiment_vars(description=DESCRIPTION,prompt=PROMPT_FILE, model=MODEL, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "# Start the run\n",
    "aiplatform.start_run(run_name)\n",
    "# Log the experiment variables\n",
    "aiplatform.log_params(exp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "3\n",
      "BatchPredictionJob projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608 current state:\n",
      "4\n",
      "BatchPredictionJob run completed. Resource name: projects/355771430623/locations/us-central1/batchPredictionJobs/6366565090992324608\n"
     ]
    }
   ],
   "source": [
    "# Construct table names\n",
    "input_table_name = f'{PROJECT_ID}.{DATASET_ID}.input{run_name}'\n",
    "output_table_name = f'{PROJECT_ID}.{DATASET_ID}.output{run_name}'\n",
    "\n",
    "# Define the table schema\n",
    "schema = [\n",
    "    bigquery.SchemaField('request', 'JSON'),\n",
    "    bigquery.SchemaField('index_no', 'INTEGER')\n",
    "]\n",
    "\n",
    "# Create the table if it doesnt exist\n",
    "table = bigquery.Table(input_table_name, schema=schema)\n",
    "if_tbl_exists(bq_client, table)\n",
    "\n",
    "# Create the pandas df that stores the requests\n",
    "batch_df = pd.DataFrame(columns=[\"request\", \"index_no\"])\n",
    "\n",
    "for t in list(predictions_df.index_no):\n",
    "    dyna_prompt = EXAMPLES + create_ex(t, False) + [str(dict(valid_example_gen(t)))]\n",
    "    df_temp = pd.DataFrame([[batch_data_create(stat_prompt, dyna_prompt, TEMPERATURE, TOP_P), t]], columns=[\"request\", \"index_no\"])\n",
    "    batch_df = pd.concat([batch_df, df_temp], ignore_index=True)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "job_config.source_format = 'CSV'\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    batch_df, input_table_name, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "# Generate the request.json for batch processing\n",
    "write_request(\"spacehackbatch_check\", MODEL, \"bq://\" + input_table_name,\n",
    "            \"bq://\" + output_table_name)\n",
    "\n",
    "# Send the batch response\n",
    "response = create_batch_prediction_job(PROJECT_ID, \"request.json\")\n",
    "# Run the batch process job and wait for completion.\n",
    "job = aiplatform.BatchPredictionJob(response[\"name\"].split(\"/\")[-1])\n",
    "job.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The query to generate a final table with results\n",
    "run_name = \"run\" + \"202412150005\"\n",
    "output_table_name = \"spacehack.outputrun202412150005\"\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{run_name}` AS\n",
    "SELECT  t1.index_no, \n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.coherence_score') AS coherence_score,\n",
    "    JSON_EXTRACT_SCALAR(JSON_EXTRACT_SCALAR(response, '$.candidates[0].content.parts[0].text'), '$.interest_score_validated') AS interest_score_coherent,\n",
    "t1.response, t1.request \n",
    "        FROM `{output_table_name}` as t1\n",
    "\"\"\"\n",
    "#Run the query\n",
    "query_job = bq_client.query(create_table_query)\n",
    "results = query_job.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the results to generate KPIs\n",
    "download_query = f\"\"\"\n",
    "SELECT index_no, coherence_score, interest_score_coherent\n",
    "FROM turan-genai-bb.spacehack.run202412150005\n",
    "\"\"\"\n",
    "pred_df = bq_client.query_and_wait(download_query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"MeerLICHT_predictions_with_Coherence_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_no</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>interest_score_coherent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>553</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>859</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>2466</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>1129</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>3071</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>866</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3219 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index_no coherence_score interest_score_coherent\n",
       "0          817               5                     Yes\n",
       "1          750               5                     Yes\n",
       "2          553               5                     Yes\n",
       "3          101               5                     Yes\n",
       "4          859               5                     Yes\n",
       "...        ...             ...                     ...\n",
       "3214       800               5                     Yes\n",
       "3215      2466               5                     Yes\n",
       "3216      1129               5                     Yes\n",
       "3217      3071               5                     Yes\n",
       "3218       866               5                     Yes\n",
       "\n",
       "[3219 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacehack",
   "language": "python",
   "name": "spacehack"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
