{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"turan-genai-bb\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"supernovadetection\" # @param {type:\"string\"}\n",
    "# Make sure that dataset is created in Big Query\n",
    "DATASET_ID = \"spacehack\" # @param {type:\"string\"}\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "import base64\n",
    "import json\n",
    "import random, os\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason, Image\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from helper_functions import build_run_batch, if_tbl_exists, create_ex, save_picture, save_prompt, build_experiment_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA = \"\"\"<PERSONA>\n",
    "You are an experienced astrophysicist tasked with evaluating the accuracy and coherence of astronomical classifications generated by a previous model. Your expertise ensures reliable judgments on how well the output aligns with the given astronomical images.\n",
    "</PERSONA>\"\"\"\n",
    "\n",
    "TASK = \"\"\"<TASK>\n",
    "Your task is to assess the coherence between the provided three images (New, Reference, and Difference) and the classification and description generated by a previous model. Additionally, you will verify if the assigned interest score is appropriate based on the description and images.\n",
    "</TASK>\\n\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTIONS = \"\"\"<INSTRUCTIONS>\n",
    "**1. Coherence Evaluation**\n",
    "- Review the classification and description given by the previous model.\n",
    "- Judge how well the model’s output matches the observed features in the images.\n",
    "- Assign a coherence score from 0 to 5:\n",
    "  - **5** - Perfectly coherent\n",
    "  - **4** - Almost entirely correct\n",
    "  - **3** - Mostly correct with some errors\n",
    "  - **2** - More incorrect than correct\n",
    "  - **1** - Majority incorrect\n",
    "  - **0** - Complete hallucination\n",
    "\n",
    "**2. Interest Score Validation**\n",
    "- Determine if the interest score given by the model is coherent with the images and description.\n",
    "- Respond with a simple **Yes** (coherent) or **No** (incoherent).\n",
    "</INSTRUCTIONS>\"\"\"\n",
    "\n",
    "\n",
    "METHOD = \"\"\"<METHOD>\n",
    "1. Examine the images and the model’s classification and description.\n",
    "2. Judge coherence, assign a score (0-5), and note any major discrepancies.\n",
    "3. Validate if the interest score is consistent with the description and images, responding with Yes or No.\n",
    "</METHOD>\n",
    "\"\"\"\n",
    "\n",
    "# Collapse the System Instructions into a single variable\n",
    "stat_prompt = PERSONA + TASK + INSTRUCTIONS + METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the dataset of images\n",
    "file_path_data = '/home/user/spacehack/MeerLICHT_dataset/MeerLICHT_images.npy'\n",
    "file_path_labels_csv = '/home/user/spacehack/MeerLICHT_dataset/MeerLICHT_labels.csv'\n",
    "predictions_file = '/home/user/spacehack/MeerLICHT_predictions.csv'\n",
    "\n",
    "# Load image triplets (New, Reference, Difference)\n",
    "triplets = np.load(file_path_data)\n",
    "\n",
    "# Load labels and predictions from CSV files\n",
    "labels_df = pd.read_csv(file_path_labels_csv)\n",
    "predictions_df = pd.read_csv(predictions_file)\n",
    "\n",
    "# Check if table already exists in BigQuery, and upload labels if not\n",
    "labels_id = \"MeerLICHT_labels_df\"\n",
    "labels_ref = bq_client.dataset(DATASET_ID).table(labels_id)\n",
    "create_table_flag = if_tbl_exists(bq_client, labels_ref)\n",
    "\n",
    "if not create_table_flag:\n",
    "    bq_client.load_table_from_dataframe(labels_df, labels_ref)\n",
    "\n",
    "# Sample indexes for saving example images\n",
    "sample_indexes = [0, 1, 3, 4, 8, 48, 77, 1179, 1180, 1181, 1191, 1193, 592, 685, 3216]\n",
    "\n",
    "# Function to save image triplets for given indexes\n",
    "def save_triplet_images(triplets, index, save_dir=\"output_images\"):\n",
    "    \"\"\"Save New, Reference, and Difference images for a given index.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    new_img, ref_img, diff_img = triplets[index]\n",
    "    # Save each image with appropriate naming\n",
    "    np.save(os.path.join(save_dir, f\"new_{index}.npy\"), new_img)\n",
    "    np.save(os.path.join(save_dir, f\"reference_{index}.npy\"), ref_img)\n",
    "    np.save(os.path.join(save_dir, f\"difference_{index}.npy\"), diff_img)\n",
    "\n",
    "# Save sample images for visual inspection\n",
    "# for i in sample_indexes:\n",
    "#     save_triplet_images(triplets, i)\n",
    "\n",
    "# Filter out corrupt data entries\n",
    "valid_indexes = np.where(~np.isnan(triplets).any(axis=(1, 2, 3)))[0]\n",
    "\n",
    "# Prepare data for LLM evaluation\n",
    "evaluation_data = []\n",
    "for idx in valid_indexes:\n",
    "    # Retrieve the corresponding prediction data by index\n",
    "    pred_row = predictions_df[predictions_df['index_no'] == idx]\n",
    "    if not pred_row.empty:\n",
    "        # Collect the triplet images and prediction details\n",
    "        new_img, ref_img, diff_img = triplets[idx]\n",
    "        explanation = pred_row['explanation'].values[0]\n",
    "        interest_score = pred_row['interest_score'].values[0]\n",
    "        \n",
    "        # Append data for LLM evaluation\n",
    "        evaluation_data.append({\n",
    "            'index_no': idx,\n",
    "            'new_image': new_img,\n",
    "            'reference_image': ref_img,\n",
    "            'difference_image': diff_img,\n",
    "            'explanation': explanation,\n",
    "            'interest_score': interest_score\n",
    "        })\n",
    "\n",
    "# Example output ready for LLM processing\n",
    "# evaluation_data is now a list of dictionaries containing image triplets and predictions details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated descriptions for the new task format\n",
    "\n",
    "## DESCRIPTION INDEX 0:\n",
    "desc1 = {\n",
    "  \"coherence_score\": 4,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1:\n",
    "desc2 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 3:\n",
    "desc3 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 4:\n",
    "desc4 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 8:\n",
    "desc5 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 48:\n",
    "desc6 = {\n",
    "  \"coherence_score\": 4,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 77:\n",
    "desc7 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1179:\n",
    "desc8 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1180:\n",
    "desc9 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1181:\n",
    "desc10 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1191:\n",
    "desc11 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 1193:\n",
    "desc12 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 592:\n",
    "desc13 = {\n",
    "  \"coherence_score\": 4,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 685:\n",
    "desc14 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "## DESCRIPTION INDEX 3216:\n",
    "desc15 = {\n",
    "  \"coherence_score\": 5,\n",
    "  \"interest_score_coherent\": \"Yes\"\n",
    "}\n",
    "\n",
    "descriptions = [\n",
    "    desc1, desc2, desc3, desc4, desc5, desc6, desc7, \n",
    "    desc8, desc9, desc10, desc11, desc12, desc13, desc14, desc15\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 15 examples for the dynamic prompt\n",
    "EXAMPLES = [\"<EXAMPLES>\\n\"]\n",
    "for i in range(len(sample_indexes)):\n",
    "    \n",
    "    str_EX = f\"\"\"Example {i+1}:\n",
    "    \"\"\"\n",
    "    all_list = create_ex(sample_indexes[i], True)\n",
    "    all_list.insert(0, str_EX)\n",
    "    all_list.append(descriptions[i])\n",
    "    all\n",
    "    for k in all_list:\n",
    "        EXAMPLES.append(k)\n",
    "EXAMPLES.append(\"\\n</EXAMPLES>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start logging the experiment\n",
    "\n",
    "## Prepare the variables\n",
    "timestamp = datetime.datetime.now()\n",
    "formatted_datetime = timestamp.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "## Log the experiments variables\n",
    "### Create the run name with timestamp\n",
    "run_name = \"run\" + formatted_datetime\n",
    "DESCRIPTION = \"\"\"Changed the instructions to give more structure.\n",
    "The hyperparameters are: \n",
    "Temperature: .8\n",
    "Top P: 1\n",
    "\"\"\" # @param {type:\"string\"}\n",
    "MODEL = \"gemini-1.5-pro-002\" # @param [gemini-1.5-pro-001\", \"gemini-1.5-flash-001\", \"gemini-1.0-pro-002\"]\n",
    "TEMPERATURE = 0.1 # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "TOP_P = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "PROMPT_FILE = save_prompt(stat_prompt + '\\n'.join([a + \"\\n\" + b + \"\\n\" for (a,b) in example_description]), run_name)\n",
    "\n",
    "# Build the experimentation variables\n",
    "exp_vars = build_experiment_vars(description=DESCRIPTION,prompt=PROMPT_FILE, model=MODEL, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "# Start the run\n",
    "aiplatform.start_run(run_name)\n",
    "# Log the experiment variables\n",
    "aiplatform.log_params(exp_vars)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
